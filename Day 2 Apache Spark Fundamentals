Day 02 â€“ Apache Spark Fundamentals

14 Days Databricks AI Challenge

ğŸ“Œ Objective

The goal of Day 02 was to understand Apache Spark fundamentals and perform basic data processing operations using Spark DataFrames on a real e-commerce dataset in Databricks.

ğŸ“Š Dataset

Source: Kaggle (E-commerce Behavior Data)

Files used: 2019-Oct.csv

Data includes:

event_type (view, cart, purchase)

product_id

user_id

price

category and timestamps

ğŸ› ï¸ Tools & Technologies

Databricks Community Edition

Apache Spark (PySpark)

Spark SQL

âœ… Tasks Performed

Loaded CSV data into a Spark DataFrame

Explored dataset using show(), count(), and printSchema()

Performed Spark transformations:

select() â€“ column selection

filter() â€“ conditional filtering

groupBy() â€“ aggregation by event type

orderBy() â€“ sorting aggregated results

Converted DataFrame into a temporary SQL view

Executed Spark SQL queries for event-level analysis

Exported aggregated results as CSV

ğŸ§  Key Learnings

Spark works on distributed data using partitions

Transformations are lazy and execute only when an action is triggered

DataFrames are optimized using Sparkâ€™s Catalyst Optimizer

Spark SQL enables analytics using SQL syntax on big data

ğŸ“‚ Output

Aggregated event-level summary exported as CSV

Spark jobs and stages observed using Databricks Spark UI

#DatabricksWithIDC #ApacheSpark #Databricks #Codebasics #IndianDataClub
